---
title: "Katana Crib Sheet"
author: "Philip J Clare"
date: "02 July 2020"
output:
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
    toc_depth: 2
    highlight: haddock
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{css, echo=FALSE}
pre {
  max-height: 450px;
  overflow-y: auto;
}

pre[class] {
  max-height: 350px;
}
```

# Katana
## Broad overview of Katana
Katana is a distributed high performance cluster computer. It is made up of a large number of relatively high performance ‘nodes’ (generally with 32 or 40 cores each), linked together by a ‘master’ node which schedules and distributes jobs.

There are two ways to run jobs on Katana:

1. Interactively <br>
This means running a job like you would on a normal PC, with a live runtime, and the ability to write and run code as you go.
2. Batch job <br>
This is a job run from a pre-written script, that does not require any intervention.

## Using Katana
For the basics of running jobs and using Katana, you can check out the information on the UNSW website: https://research.unsw.edu.au/katana

However, there are some things that are worth considering, in addition to the basics of getting a job running:

* Run smaller tests first. In addition to making sure there are no issues with your code, this can also help give you a better idea of exactly how many resources to request. If you ask for 100gb of memory, but the end-of-job email says the job only used 30gb, you can safely lower your request. This will make it easier for the scheduler to fit in your job.
* Think carefully about the best way to split the job up to make it most efficient.
* Be careful of replicability/random seeds. A lot of processes (eg multiple imputation) use random seeds to ensure both (quasi)randomness and replicability. This means that if you split the job up across nodes, you need to be very careful about how seeds are set to ensure that the random processes are independent but replicable.

## Parallel Processing
The main advantage of Katana is parallel processing. Each individual core on Katana nodes are not particularly different to a standard PC (some may be faster, but some are slower). But Katana allows jobs to potentially use dozens of cores simultaneously.

There are two main ways to parallelise a job using a cluster computer:

1. In-program parallelisation, for example using the R base package ‘parallel’. It is worth noting that, while a multi-core version of Stata exists, it is NOT installed on Katana, so Stata on Katana is limited to single core processes (it is also not a very recent version).
2. Splitting the job across nodes using an array job which calls runs the same script multiple times (potentially with parameters)
There are advantages and disadvantages to each, but the main ones are:

|              | In-program parallelisation           | Array jobs                          |
|------------- | ------------------------------------ | ------------------------------------|
|Advantages    | Job script can be written and tested on local PC – only major difference is the number of cores available. | Jobs can be split into smaller individual jobs, which can be distributed across nodes, using any available cores as they come up. |
|Disadvantages | Requires all the cores to be on the same node, which can affect wait times (and potentially viability of the job). | Requires more setup, because job script has to be ‘generic’.<br>Can sometimes take longer because of overcomputing (see below). |

## Examples
I will show two examples of the use of Katana for parallel processing. Code for both examples is available on [Github](https://github.com/philipclare/katana).

### In-program Parallelisation
R has a range of parallel processing options, including the base package ‘parallel’, which contains a number of functions designed for parallel processing. In addition, there are quite a few R packages that are designed to work in parallel.

One example of this is multiple imputation via parallel processing, using the function ‘parlmice’ from the package ‘miceadds’. This function works similarly to the function ‘mice’, performing multiple imputation using a range of different possible imputation methods, but in parallel rather than serial.
However, generic parallel wrappers are also available- for example, the function ‘parLapply’ is a parallel extension of ‘lapply’, designed to work with clusters defined by ‘parallel’ (see: [Package ‘parallel’](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf)). Similarly, the function ‘foreach’ can be applied in parallel using the binary operator %dopar% (see: [Getting Started with doParallel and foreach](https://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf)).

This kind of parallel processing can also be used on a standard workstation (although parallel processing is somewhat less efficient in Windows than *nix. Thus the primary advantage of Katana for in-program parallelisation is the number of cores available on Katana nodes (up to 40).

Example 1 shows an example of in-program parallelisation to perform a relatively simply imputation using parlmice.

#### Example 1 PBS Script
```{r, code = readLines("https://philipclare.github.io/katana/Code/E1-Imputation.pbs"), eval = FALSE}
```

#### Example 1 R Code
```{r, code = readLines("https://philipclare.github.io/katana/Code/E1-Imputation.R"), eval = FALSE}
```

### Array job
For very large jobs (or jobs that do not use R), in-program parallel processing is often simply not possible. For example, take cross-validation of a machine learning algorithm, which might need to be run 1000 times, and take several hours to run each time. Even using all 40 CPUS on the NDARC node, that job could take weeks to run. However, it may be possible to split the processes so that each iteration runs separately, on as many available CPUs at a time. In reality, Katana will allow you to use up to 150 cores at a time – far more than the 40 available on any given node.

This does require some work. The code itself must be written to be ‘generic’ – that is, the only difference between the iterations is something that can be passed to the code via a parameter. Consider a very simple example, where you want to run the same analysis on 1000 different datasets – in that case, the code itself would be the same, and only the number of the dataset to load would differ – a parameter which can be defined by Katana and passed to the program script.

Note that this can create difficulties – in particular, with any operation that relies on random numbers. Take, for example, a Monte-Carlo simulation – where the code is run numerous times, and the only difference between iterations is the random number generate ‘state’, which means that numbers are as random and independent as possible. It is not a trivial matter to set seeds in such a way as to guarantee that both 1) the numbers generated by each iteration are replicable, and 2) that the numbers generated are independent (ie that the same range state is not used twice). There are ways to handle issues like this, but they require care and thought.

It is also worth remembering that there is potentially overhead in creating and initialising a new node – loading packages, defining parameters, etc. So it is important to arrive at a balance between parallel and serial processes. As a general rule of thumb, I suggest trying to set up each node so that the walltime is approximately 10 hours (and importantly no more than 12 hours, so that walltime can be set at 12 hours, which will minimise wait time). If each individual process takes approximate 5 minutes, that would mean each node could perform 120 processes in serial, although it might be easier to round down to 100.

As an example of this kind of job, take a very simple Monte-Carlo simulation. This simulation creates a random dataset based on a mixed-effect model. It then runs a random effects model on that data using ‘lmer’ from the package ‘lme4’, and save the coefficient and standard error of just the fixed effect. In this example, each simulation iteration is very quick, so runtime is well under that (in reality, this simulation is not one that I would need to run on Katana), but gives a relatively simple example of an array job.

#### Example 2 PBS Script

```{r, code = readLines("https://philipclare.github.io/katana/Code/E2-Monte-Carlo.pbs"), eval = FALSE}
```

#### Example 2 R Code
```{r, code = readLines("https://philipclare.github.io/katana/Code/E2-Monte-Carlo.R"), eval = FALSE}
```

## PBS Script Options

Flag | Use/Description
-----|--------
-I	 | Creates an interactive job
-N	 | Job name (this will appear in the scheduler and any output)
-l	 | Resources required. This should include the number of CPUS, amount of memory and wall time per node. Can be set on separate lines, or sequentially, separated by colons. Eg: <br> #PBS -l select=1:ncpus=10:mem=40gb
-M	 | Email address to send reports etc
-m	 | Email options. Can include any of three options:<br>a – abort<br>b – begin<br>e – end 
-j	 | Typically as ‘-j oe’, this tells Katana to join the output and error files into a single output
-k	 | Tells Katana when to create output. Using the flag ‘oed’ tells Katana to start creating output when the job starts (the default is to save all output when the job ends).
-J	 | Defines the range of an array job, which can be passed to scripts using the macro ‘$PBS_ARRAY_INDEX’. That is, setting J as 1-10 will create an array of 10 jobs, with different values of J from 1 to 10.

## Other tips
* Katana creates a system variable that contains the number of CPUs in the job. This can be called in R using ‘Sys.getenv(‘NCPUS’)’ to define the number of cores dynamically when creating clusters using ‘parallel’.
* If you’re going to be using Katana regularly, I’d suggest creating a folder in outlook and automatically sending all emails from ‘adm@kman.restech.unsw.edu.au’ to it – you could get a lot of emails, especially if you use array jobs.


